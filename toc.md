---
layout: course_page
title: Table of Contents
---

# Table of Contents

[0. Introduction](section/0/notes.md) | [Slides](section/0/slides.pdf) | [Notebook](https://colab.research.google.com/github/damek/STAT-4830/blob/main/section/0/notebook.ipynb)
   > Course content, a deliverable, and spam classification in PyTorch.

[1. Basic linear Algebra in PyTorch](section/1/notes.md) | [Slides](section/1/slides.pdf) | [Notebook](https://colab.research.google.com/github/damek/STAT-4830/blob/main/section/1/notebook.ipynb) | [Live Demo](https://colab.research.google.com/github/damek/STAT-4830/blob/main/section/1/live-demo.ipynb)
   > Basic linear algebra operations in PyTorch.

[2. Linear Regression: Direct Methods](section/2/notes.md) | [Slides](section/2/slides.pdf) | [Notebook](https://colab.research.google.com/github/damek/STAT-4830/blob/main/section/2/notebook.ipynb)
   > Direct methods for solving least squares problems, comparing LU and QR factorization.

[3. Linear Regression: Gradient Descent](section/3/notes.md) | [Slides](section/3/slides.pdf) | [Notebook](https://colab.research.google.com/github/damek/STAT-4830/blob/main/section/3/notebook.ipynb)
   > Linear regression via gradient descent. 

[4. How to compute gradients in PyTorch](section/4/notes.md) | [Slides](section/4/slides.pdf) | [Notebook](https://colab.research.google.com/github/damek/STAT-4830/blob/main/section/4/notebook.ipynb)
   > Introduction to PyTorch's automatic differentiation system.

[5. How to think about derivatives through best linear approximation](section/5/notes.md)
   > How to think about derivatives through best linear approximation.

[6. Stochastic gradient descent: A first look](section/6/notes.md)
   > A first look at stochastic gradient descent through the mean estimation problem.

[7. Stochastic gradient descent: insights from the Noisy Quadratic Model](section/7/notes.md)
   > When should we use exponential moving averages, momentum, and preconditioning?

[8. Stochastic Gradient Descent: The general problem and implementation details](section/8/notes.md) | [Notebook](https://colab.research.google.com/github/damek/STAT-4830/blob/main/section/8/notebook.ipynb)
   > Stochastic optimization problems, SGD, tweaks, and implementation in PyTorch

[9. Adaptive Optimization Methods](section/9/notes.md) | [Notebook](https://colab.research.google.com/github/damek/STAT-4830/blob/main/section/9/notebook.ipynb)
   > Intro to adaptive optimization methods: Adagrad, Adam, and AdamW.

[10. Benchmarking Optimizers: Challenges and Some Empirical Results](section/10/notes.md) 
   > How do we compare optimizers for deep learning? 

[11. A Playbook for Tuning Deep Learning Models](section/11/notes.md)
   > A systematic process for [tuning deep learning models](https://github.com/google-research/tuning_playbook)

[12. Scaling Transformers: Parallelism Strategies from the Ultrascale Playbook](section/12/notes.md)
   > How do we scale training of transformers to 100s of billions of parameters?
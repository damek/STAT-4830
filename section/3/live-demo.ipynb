{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. PyTorch tour: tensors, graphs, operations, efficiency, dtypes, devices\n",
        "\n",
        "This section is deliberately code-heavy.\n",
        "\n",
        "In class, I will run these examples live. In the notes, each concept is paired with a minimal runnable snippet.\n",
        "\n",
        "### 4.1 Conventions for the code blocks\n",
        "\n",
        "- Every snippet is meant to run in a fresh Python process.\n",
        "- If a line prints something, I show the output as a comment.\n",
        "- If output depends on hardware (CPU vs GPU) or timing, I label it as example output.\n",
        "\n",
        "### 4.2 Tensors in higher dimensions: creation\n",
        "\n",
        "A tensor is a container for numbers, plus metadata:\n",
        "\n",
        "- shape,\n",
        "- dtype,\n",
        "- device,\n",
        "- (optionally) autodiff tracking.\n",
        "\n",
        "#### From Python lists\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "7a50c64c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor([1., 2., 3.])\n",
            "A:\n",
            " tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "x.shape: torch.Size([3])\n",
            "A.shape: torch.Size([2, 3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "A = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "\n",
        "print(\"x:\", x)\n",
        "print(\"A:\\n\", A)\n",
        "print(\"x.shape:\", x.shape)\n",
        "print(\"A.shape:\", A.shape)\n",
        "\n",
        "# Output:\n",
        "# x: tensor([1., 2., 3.])\n",
        "# A:\n",
        "#  tensor([[1, 2, 3],\n",
        "#          [4, 5, 6]])\n",
        "# x.shape: torch.Size([3])\n",
        "# A.shape: torch.Size([2, 3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d86ab6a5",
      "metadata": {},
      "source": [
        "\n",
        "#### Indexing with `[]` and `:`\n",
        "\n",
        "Use `[]` to access elements and slices. `:` means “all entries along that axis.” `-1` means “last.”\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "d865c976",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x[0]: tensor(10.)\n",
            "x[-1]: tensor(40.)\n",
            "x[1:3]: tensor([20., 30.])\n",
            "A[0, 1]: tensor(2)\n",
            "A[1, :]: tensor([4, 5, 6])\n",
            "A[:, -1]: tensor([3, 6, 9])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([10.0, 20.0, 30.0, 40.0])\n",
        "A = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6],\n",
        "                  [7, 8, 9]])\n",
        "\n",
        "print(\"x[0]:\", x[0])\n",
        "print(\"x[-1]:\", x[-1])\n",
        "print(\"x[1:3]:\", x[1:3])\n",
        "print(\"A[0, 1]:\", A[0, 1])\n",
        "print(\"A[1, :]:\", A[1, :])\n",
        "print(\"A[:, -1]:\", A[:, -1])\n",
        "\n",
        "# Output:\n",
        "# x[0]: tensor(10.)\n",
        "# x[-1]: tensor(40.)\n",
        "# x[1:3]: tensor([20., 30.])\n",
        "# A[0, 1]: tensor(2)\n",
        "# A[1, :]: tensor([4, 5, 6])\n",
        "# A[:, -1]: tensor([3, 6, 9])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acafd853",
      "metadata": {},
      "source": [
        "\n",
        "#### From and to NumPy\n",
        "\n",
        "Two important facts:\n",
        "\n",
        "1. `torch.from_numpy(np_array)` shares memory with NumPy (zero-copy).\n",
        "2. `.numpy()` shares memory with the tensor when the tensor is on CPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "00764736",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a_t:\n",
            " tensor([[1., 2.],\n",
            "        [3., 4.]])\n",
            "same memory? True\n",
            "after modifying numpy, a_t:\n",
            " tensor([[999.,   2.],\n",
            "        [  3.,   4.]])\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "a_np = np.array([[1.0, 2.0],\n",
        "                 [3.0, 4.0]], dtype=np.float32)\n",
        "\n",
        "a_t = torch.from_numpy(a_np)\n",
        "\n",
        "print(\"a_t:\\n\", a_t)\n",
        "print(\"same memory?\", a_t.data_ptr() == a_np.__array_interface__[\"data\"][0])\n",
        "\n",
        "a_np[0, 0] = 999.0\n",
        "print(\"after modifying numpy, a_t:\\n\", a_t)\n",
        "\n",
        "# Output:\n",
        "# a_t:\n",
        "#  tensor([[1., 2.],\n",
        "#          [3., 4.]])\n",
        "# same memory? True\n",
        "# after modifying numpy, a_t:\n",
        "#  tensor([[999.,   2.],\n",
        "#          [  3.,   4.]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "948ca790",
      "metadata": {},
      "source": [
        "\n",
        "One might think `.numpy()` is a “copy out.” However, on CPU it is typically a view into the same storage. If you need a copy, use `.clone()` first.\n",
        "\n",
        "#### Random tensors\n",
        "\n",
        "Gaussian and uniform are the workhorses:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "ad108b91",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "g:\n",
            " tensor([[ 1.5410, -0.2934, -2.1788],\n",
            "        [ 0.5684, -1.0845, -1.3986]])\n",
            "u:\n",
            " tensor([[0.0223, 0.1689, 0.2939],\n",
            "        [0.5185, 0.6977, 0.8000]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "g = torch.randn(2, 3)   # N(0,1)\n",
        "u = torch.rand(2, 3)    # Unif[0,1)\n",
        "\n",
        "print(\"g:\\n\", g)\n",
        "print(\"u:\\n\", u)\n",
        "\n",
        "# Output:\n",
        "# g:\n",
        "#  tensor([[ 1.5410, -0.2934, -2.1788],\n",
        "#          [ 0.5684, -1.0845, -1.3986]])\n",
        "# u:\n",
        "#  tensor([[0.4963, 0.7682, 0.0885],\n",
        "#          [0.1320, 0.3074, 0.6341]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f6d3886",
      "metadata": {},
      "source": [
        "\n",
        "#### `empty`: uninitialized memory\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "4cd541f9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.empty(2, 3)\n",
        "print(x)\n",
        "\n",
        "# Output:\n",
        "# tensor([[... random-looking values ...]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b670c34",
      "metadata": {},
      "source": [
        "\n",
        "Why would you ever do this?\n",
        "\n",
        "- `torch.empty` allocates memory but does not fill it.\n",
        "- If you are about to overwrite every entry anyway, `empty` avoids an unnecessary initialization pass.\n",
        "\n",
        "A common pattern is:\n",
        "\n",
        "- allocate with `empty`,\n",
        "- fill with some computation.\n",
        "\n",
        "#### Zeros and ones\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "c5ff4ab9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zeros:\n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "ones:\n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "z = torch.zeros(2, 3)\n",
        "o = torch.ones(2, 3)\n",
        "\n",
        "print(\"zeros:\\n\", z)\n",
        "print(\"ones:\\n\", o)\n",
        "\n",
        "# Output:\n",
        "# zeros:\n",
        "#  tensor([[0., 0., 0.],\n",
        "#          [0., 0., 0.]])\n",
        "# ones:\n",
        "#  tensor([[1., 1., 1.],\n",
        "#          [1., 1., 1.]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01ad3068",
      "metadata": {},
      "source": [
        "\n",
        "#### Reference/clone\n",
        "\n",
        "Two things that look similar in Python are not the same in PyTorch:\n",
        "\n",
        "- assignment creates a reference (same storage, same graph),\n",
        "- `clone()` copies data (new storage, graph connection preserved).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "9e1f5e6c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x.data_ptr: 5208594112\n",
            "y.data_ptr: 5208594112\n",
            "z.data_ptr: 5208484928\n",
            "after y[0]=999:\n",
            "x: tensor([999.,   2.,   3.])\n",
            "y: tensor([999.,   2.,   3.])\n",
            "z: tensor([1., 2., 3.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "y = x              # reference (no new memory)\n",
        "z = x.clone()      # data copy (new memory)\n",
        "\n",
        "print(\"x.data_ptr:\", x.data_ptr())\n",
        "print(\"y.data_ptr:\", y.data_ptr())\n",
        "print(\"z.data_ptr:\", z.data_ptr())\n",
        "\n",
        "y[0] = 999.0\n",
        "print(\"after y[0]=999:\")\n",
        "print(\"x:\", x)\n",
        "print(\"y:\", y)\n",
        "print(\"z:\", z)\n",
        "\n",
        "# Output:\n",
        "# x.data_ptr:  ...\n",
        "# y.data_ptr:  ... (same as x)\n",
        "# z.data_ptr:  ... (different)\n",
        "# after y[0]=999:\n",
        "# x: tensor([999.,   2.,   3.])\n",
        "# y: tensor([999.,   2.,   3.])\n",
        "# z: tensor([1., 2., 3.])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da8fa64a",
      "metadata": {},
      "source": [
        "\n",
        "So:\n",
        "\n",
        "- “copy” in casual Python talk often means “another name for the same object.”\n",
        "- `clone()` means “a new tensor with its own storage.”\n",
        "- if `x` is tracked, `x.clone()` is still attached to the computation graph; use `x.detach().clone()` to copy without the graph.\n",
        "\n",
        "### 4.3 Tensor manipulation: shape, reshape, view, concat, squeeze\n",
        "\n",
        "#### `.numel` and `.shape`\n",
        "\n",
        "Access to tensor shape and number of elements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "53dbfcc6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A.shape: torch.Size([2, 3, 4])\n",
            "A.numel(): 24\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "A = torch.randn(2, 3, 4)\n",
        "print(\"A.shape:\", A.shape)\n",
        "print(\"A.numel():\", A.numel())\n",
        "\n",
        "# Output:\n",
        "# A.shape: torch.Size([2, 3, 4])\n",
        "# A.numel(): 24\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d788cba3",
      "metadata": {},
      "source": [
        "\n",
        "#### Memory layout: contiguous vs non-contiguous\n",
        "\n",
        "Aside: this will help with `reshape`/`view` below, and it will matter for efficiency later. PyTorch is row-major (C-order) by default, so the last index changes fastest in memory.\n",
        "\n",
        "PyTorch tensors have **strides**, which describe how to step through memory to move along each dimension. Concretely, the stride tuple tells you how many *elements* you jump in the underlying 1D storage when you increment an index by 1 along each axis.\n",
        "\n",
        "Example: for a 3×4 row-major tensor, a stride of `(4, 1)` means “move 4 elements to go down one row, move 1 element to go right one column.”\n",
        "\n",
        "The default layout for a 2D tensor is row-major contiguous in the sense that the last dimension is contiguous.\n",
        "\n",
        "Transpose changes the stride pattern without moving data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "7a861cab",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A.shape: torch.Size([3, 4]) stride: (4, 1) contiguous: True\n",
            "AT.shape: torch.Size([4, 3]) stride: (1, 4) contiguous: False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "A = torch.arange(12).reshape(3, 4)\n",
        "AT = A.t()\n",
        "\n",
        "print(\"A.shape:\", A.shape, \"stride:\", A.stride(), \"contiguous:\", A.is_contiguous())\n",
        "print(\"AT.shape:\", AT.shape, \"stride:\", AT.stride(), \"contiguous:\", AT.is_contiguous())\n",
        "\n",
        "# Output:\n",
        "# A.shape: torch.Size([3, 4]) stride: (4, 1) contiguous: True\n",
        "# AT.shape: torch.Size([4, 3]) stride: (1, 4) contiguous: False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3e6c7fb",
      "metadata": {},
      "source": [
        "\n",
        "**Figure 3.5: row-major vs column-major intuition.**  \n",
        "![Row-major vs column-major memory layout](figures/memory_layout_row_vs_col.png)\n",
        "*Figure 3.5: A 2D array is stored in 1D memory. Row-major means row entries are contiguous. Column-major means column entries are contiguous. Strides are the precise way to describe both.*\n",
        "\n",
        "We will talk about efficiency considerations later, but an actionable rule is:\n",
        "\n",
        "- if you see `is_contiguous()` is `False` and performance matters, consider making a contiguous copy with `.contiguous()` at an appropriate point.\n",
        "\n",
        "#### `reshape`: change shape (maybe copy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "b4ede964",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
            "A:\n",
            " tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n",
            "A.shape: torch.Size([3, 4])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.arange(12)     # 0,1,...,11\n",
        "A = x.reshape(3, 4)\n",
        "\n",
        "print(\"x:\", x)\n",
        "print(\"A:\\n\", A)\n",
        "print(\"A.shape:\", A.shape)\n",
        "\n",
        "# Output:\n",
        "# x: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
        "# A:\n",
        "#  tensor([[ 0,  1,  2,  3],\n",
        "#          [ 4,  5,  6,  7],\n",
        "#          [ 8,  9, 10, 11]])\n",
        "# A.shape: torch.Size([3, 4])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd7d91ce",
      "metadata": {},
      "source": [
        "\n",
        "You can use `-1` to infer a dimension:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "4138c010",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "B.shape: torch.Size([2, 6])\n",
            "B:\n",
            " tensor([[ 0,  1,  2,  3,  4,  5],\n",
            "        [ 6,  7,  8,  9, 10, 11]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.arange(12)\n",
        "B = x.reshape(2, -1)\n",
        "\n",
        "print(\"B.shape:\", B.shape)\n",
        "print(\"B:\\n\", B)\n",
        "\n",
        "# Output:\n",
        "# B.shape: torch.Size([2, 6])\n",
        "# B:\n",
        "#  tensor([[ 0,  1,  2,  3,  4,  5],\n",
        "#          [ 6,  7,  8,  9, 10, 11]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6b13306",
      "metadata": {},
      "source": [
        "\n",
        "#### `view`: change shape without copying (requires contiguous layout)\n",
        "\n",
        "A `view` is a different interpretation of the same memory.\n",
        "\n",
        "One might think `reshape` and `view` are identical. However:\n",
        "\n",
        "- `view` requires the tensor to be contiguous in memory,\n",
        "- `reshape` will return a view when possible, and otherwise it will copy.\n",
        "\n",
        "So when the tensor is contiguous, `view` and `reshape` are identical: both return a view with no data copy.\n",
        "\n",
        "A standard way to make a non-contiguous tensor is transpose.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "64bc4657",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A.is_contiguous(): True\n",
            "AT.is_contiguous(): False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "A = torch.arange(12).reshape(3, 4)\n",
        "AT = A.t()   # transpose: shape (4,3)\n",
        "\n",
        "print(\"A.is_contiguous():\", A.is_contiguous())\n",
        "print(\"AT.is_contiguous():\", AT.is_contiguous())\n",
        "\n",
        "# Output:\n",
        "# A.is_contiguous(): True\n",
        "# AT.is_contiguous(): False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faaa7360",
      "metadata": {},
      "source": [
        "\n",
        "Now compare `view` vs `reshape`:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "a60bbacd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "view failed: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n",
            "reshape worked: tensor([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "A = torch.arange(12).reshape(3, 4)\n",
        "AT = A.t()\n",
        "\n",
        "try:\n",
        "    v = AT.view(-1)\n",
        "    print(\"view worked:\", v)\n",
        "except RuntimeError as e:\n",
        "    print(\"view failed:\", e)\n",
        "\n",
        "r = AT.reshape(-1)\n",
        "print(\"reshape worked:\", r)\n",
        "\n",
        "# Output:\n",
        "# view failed: view size is not compatible with input tensor's size and stride ...\n",
        "# reshape worked: tensor([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d60732a5",
      "metadata": {},
      "source": [
        "\n",
        "#### Concatenation: vertical and horizontal\n",
        "\n",
        "For 2D tensors:\n",
        "\n",
        "- concatenate “vertically” means `dim=0` (stack rows),\n",
        "- concatenate “horizontally” means `dim=1` (stack columns).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "id": "b0deb805",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "V.shape: torch.Size([4, 3])\n",
            "V:\n",
            " tensor([[ 1,  2,  3],\n",
            "        [ 4,  5,  6],\n",
            "        [10, 20, 30],\n",
            "        [40, 50, 60]])\n",
            "H.shape: torch.Size([2, 6])\n",
            "H:\n",
            " tensor([[ 1,  2,  3, 10, 20, 30],\n",
            "        [ 4,  5,  6, 40, 50, 60]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "A = torch.tensor([[1, 2, 3],\n",
        "                  [4, 5, 6]])\n",
        "B = torch.tensor([[10, 20, 30],\n",
        "                  [40, 50, 60]])\n",
        "\n",
        "V = torch.cat([A, B], dim=0)  # vertical\n",
        "H = torch.cat([A, B], dim=1)  # horizontal\n",
        "\n",
        "print(\"V.shape:\", V.shape)\n",
        "print(\"V:\\n\", V)\n",
        "print(\"H.shape:\", H.shape)\n",
        "print(\"H:\\n\", H)\n",
        "\n",
        "# Output:\n",
        "# V.shape: torch.Size([4, 3])\n",
        "# V:\n",
        "#  tensor([[ 1,  2,  3],\n",
        "#          [ 4,  5,  6],\n",
        "#          [10, 20, 30],\n",
        "#          [40, 50, 60]])\n",
        "# H.shape: torch.Size([2, 6])\n",
        "# H:\n",
        "#  tensor([[ 1,  2,  3, 10, 20, 30],\n",
        "#          [ 4,  5,  6, 40, 50, 60]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acb37740",
      "metadata": {},
      "source": [
        "\n",
        "Shape compatibility matters:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "9daef950",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "concat failed: Sizes of tensors must match except in dimension 1. Expected size 2 but got size 3 for tensor number 1 in the list.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "A = torch.zeros(2, 3)\n",
        "C = torch.zeros(3, 3)\n",
        "\n",
        "try:\n",
        "    torch.cat([A, C], dim=1)\n",
        "except RuntimeError as e:\n",
        "    print(\"concat failed:\", e)\n",
        "\n",
        "# Output:\n",
        "# concat failed: Sizes of tensors must match except in dimension 1 ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b60afad3",
      "metadata": {},
      "source": [
        "\n",
        "#### `squeeze` and `unsqueeze`\n",
        "\n",
        "Sometimes your tensor has a dimension of size 1 that you want to remove (squeeze), or you want to add a size-1 dimension to make shapes align (unsqueeze).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "99ee2f5d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "v.shape: torch.Size([3])\n",
            "v_row.shape: torch.Size([1, 3])\n",
            "v_col.shape: torch.Size([3, 1])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "v = torch.tensor([1.0, 2.0, 3.0])   # shape (3,)\n",
        "v_row = v.unsqueeze(0)              # shape (1,3)\n",
        "v_col = v.unsqueeze(1)              # shape (3,1)\n",
        "\n",
        "print(\"v.shape:\", v.shape)\n",
        "print(\"v_row.shape:\", v_row.shape)\n",
        "print(\"v_col.shape:\", v_col.shape)\n",
        "\n",
        "# Output:\n",
        "# v.shape: torch.Size([3])\n",
        "# v_row.shape: torch.Size([1, 3])\n",
        "# v_col.shape: torch.Size([3, 1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7192c47",
      "metadata": {},
      "source": [
        "\n",
        "A practical use: concatenating a single row onto a matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "id": "729c8942",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A2.shape: torch.Size([3, 3])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [1., 2., 3.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "A = torch.zeros(2, 3)\n",
        "v = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "A2 = torch.cat([A, v.unsqueeze(0)], dim=0)\n",
        "print(\"A2.shape:\", A2.shape)\n",
        "print(A2)\n",
        "\n",
        "# Output:\n",
        "# A2.shape: torch.Size([3, 3])\n",
        "# tensor([[0., 0., 0.],\n",
        "#         [0., 0., 0.],\n",
        "#         [1., 2., 3.]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37c11248",
      "metadata": {},
      "source": [
        "\n",
        "### 4.4 A larger tour of the PyTorch API\n",
        "\n",
        "This is not an exhaustive tour. It is a “things you will use constantly later” tour.\n",
        "\n",
        "#### Entrywise operations\n",
        "\n",
        "Any scalar function can be applied entrywise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "id": "3d84bef2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "exp: tensor([ 2.7183,  7.3891, 20.0855])\n",
            "log: tensor([0.0000, 0.6931, 1.0986])\n",
            "sin: tensor([0.8415, 0.9093, 0.1411])\n",
            "x**2: tensor([1., 4., 9.])\n",
            "x*x: tensor([1., 4., 9.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "print(\"exp:\", torch.exp(x))\n",
        "print(\"log:\", torch.log(x))\n",
        "print(\"sin:\", torch.sin(x))\n",
        "print(\"x**2:\", x**2)\n",
        "print(\"x*x:\", x*x)\n",
        "\n",
        "# Output:\n",
        "# exp: tensor([ 2.7183,  7.3891, 20.0855])\n",
        "# log: tensor([0.0000, 0.6931, 1.0986])\n",
        "# sin: tensor([0.8415, 0.9093, 0.1411])\n",
        "# x**2: tensor([1., 4., 9.])\n",
        "# x*x: tensor([1., 4., 9.])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c15f85be",
      "metadata": {},
      "source": [
        "\n",
        "Two ways to square a vector (`x**2` vs `x*x`) are equivalent here.\n",
        "\n",
        "#### Reductions replace loops\n",
        "\n",
        "Reductions compute aggregates over dimensions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "id": "2f52a094",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sum all: 10.0\n",
            "sum dim=0: tensor([4., 6.])\n",
            "sum dim=1: tensor([3., 7.])\n",
            "mean: 2.5\n",
            "max: 4.0\n",
            "argmax (flattened): 3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "A = torch.tensor([[1.0, 2.0],\n",
        "                  [3.0, 4.0]])\n",
        "\n",
        "print(\"sum all:\", A.sum().item())\n",
        "print(\"sum dim=0:\", A.sum(dim=0))\n",
        "print(\"sum dim=1:\", A.sum(dim=1))\n",
        "print(\"mean:\", A.mean().item())\n",
        "print(\"max:\", A.max().item())\n",
        "print(\"argmax (flattened):\", A.argmax().item())\n",
        "\n",
        "# Output:\n",
        "# sum all: 10.0\n",
        "# sum dim=0: tensor([4., 6.])\n",
        "# sum dim=1: tensor([3., 7.])\n",
        "# mean: 2.5\n",
        "# max: 4.0\n",
        "# argmax (flattened): 3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04ed1053",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "#### Basic linear algebra\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "id": "dd41432d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A.T:\n",
            " tensor([[1., 3.],\n",
            "        [2., 4.]])\n",
            "A @ x: tensor([ 50., 110.])\n",
            "x @ x: tensor(500.)\n",
            "||x||: tensor(22.3607)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "A = torch.tensor([[1.0, 2.0],\n",
        "                  [3.0, 4.0]])\n",
        "x = torch.tensor([10.0, 20.0])\n",
        "\n",
        "print(\"A.T:\\n\", A.t())\n",
        "print(\"A @ x:\", (A @ x))\n",
        "print(\"x @ x:\", (x @ x))   # inner product for 1D tensors\n",
        "print(\"||x||:\", torch.norm(x))\n",
        "\n",
        "# Output:\n",
        "# A.T:\n",
        "#  tensor([[1., 3.],\n",
        "#          [2., 4.]])\n",
        "# A @ x: tensor([ 50., 110.])\n",
        "# x @ x: tensor(500.)\n",
        "# ||x||: tensor(...)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbd94675",
      "metadata": {},
      "source": [
        "\n",
        "#### Matrix-matrix multiplication\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "f4115bc9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C.shape: torch.Size([2, 4])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "A = torch.randn(2, 3)\n",
        "B = torch.randn(3, 4)\n",
        "C = A @ B\n",
        "\n",
        "print(\"C.shape:\", C.shape)\n",
        "\n",
        "# Output:\n",
        "# C.shape: torch.Size([2, 4])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a091bb3d",
      "metadata": {},
      "source": [
        "\n",
        "What about “multiplying tensors” beyond matrices?\n",
        "\n",
        "- `@` and `torch.matmul` generalize matrix multiplication to batched dimensions.\n",
        "- The semantics are precise, but you need to know what dimensions are treated as batch vs matrix dimensions.\n",
        "- Later we will use `einsum` because it forces you to be explicit about index contractions.\n",
        "\n",
        "#### Logical ops and masking\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "c482d37a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mask: tensor([False, False, False,  True,  True])\n",
            "x[mask]: tensor([1., 2.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
        "mask = x > 0\n",
        "\n",
        "print(\"mask:\", mask)\n",
        "print(\"x[mask]:\", x[mask])\n",
        "\n",
        "# Output:\n",
        "# mask: tensor([False, False, False,  True,  True])\n",
        "# x[mask]: tensor([1., 2.])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "713e44a4",
      "metadata": {},
      "source": [
        "\n",
        "#### Loss functions: `torch.nn.functional`\n",
        "\n",
        "PyTorch has common losses implemented and numerically stabilized.\n",
        "\n",
        "Two functions you should know exist:\n",
        "\n",
        "- `torch.logsumexp`,\n",
        "- `torch.nn.functional.cross_entropy` and `binary_cross_entropy_with_logits`.\n",
        "\n",
        "#### Logistic regression for spam vs not-spam (vectorized)\n",
        "\n",
        "We will write the whole “model + loss” in code.\n",
        "\n",
        "Setup:\n",
        "\n",
        "- data matrix $X \\in \\mathbb{R}^{n \\times d}$,\n",
        "- labels $y \\in \\{0,1\\}^n$,\n",
        "- parameter vector $w \\in \\mathbb{R}^d$,\n",
        "- logits $z = Xw$,\n",
        "- loss = binary cross entropy on logits.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "1fcea99b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits.shape: torch.Size([8])\n",
            "loss: 1.3334670066833496\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "n = 8\n",
        "d = 5\n",
        "\n",
        "X = torch.randn(n, d)\n",
        "y = torch.randint(low=0, high=2, size=(n,)).float()  # 0/1 labels\n",
        "\n",
        "w = torch.randn(d, requires_grad=True)\n",
        "\n",
        "logits = X @ w                      # shape (n,)\n",
        "loss = F.binary_cross_entropy_with_logits(logits, y)\n",
        "\n",
        "print(\"logits.shape:\", logits.shape)\n",
        "print(\"loss:\", loss.item())\n",
        "\n",
        "# Output:\n",
        "# logits.shape: torch.Size([8])\n",
        "# loss: ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a897d2",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "One thing you should note: We used `binary_cross_entropy_with_logits`, not “sigmoid then BCE.” The logits version is more numerically stable.\n",
        "\n",
        "\n",
        "#### `torch.nn`: a tiny two-layer network\n",
        "\n",
        "A linear model predicts with $x^\\top w$. A neural network predicts with a nonlinear mapping.\n",
        "\n",
        "We will build:\n",
        "\n",
        "- Linear layer: $\\mathbb{R}^d \\to \\mathbb{R}^h$,\n",
        "- ReLU,\n",
        "- Linear layer: $\\mathbb{R}^h \\to \\mathbb{R}$.\n",
        "\n",
        "**Figure 3.3: two-layer network architecture.**  \n",
        "![Two-layer network](figures/two_layer_network.png)\n",
        "*Figure 3.3: A simple 2-layer MLP: linear layer, ReLU, linear layer. Later we will replace this with transformer blocks.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "id": "dc393eda",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits.shape: torch.Size([8])\n",
            "loss: 0.6901422142982483\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "n = 8\n",
        "d = 5\n",
        "h = 16\n",
        "\n",
        "X = torch.randn(n, d)\n",
        "y = torch.randint(low=0, high=2, size=(n,)).float()\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(d, h),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(h, 1),\n",
        ")\n",
        "\n",
        "logits = model(X).squeeze(1)     # shape (n,)\n",
        "loss = F.binary_cross_entropy_with_logits(logits, y)\n",
        "\n",
        "print(\"logits.shape:\", logits.shape)\n",
        "print(\"loss:\", loss.item())\n",
        "\n",
        "# Output:\n",
        "# logits.shape: torch.Size([8])\n",
        "# loss: ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1dcfb0c",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "### 4.5 Computational graphs in higher dimensions\n",
        "\n",
        "We're going to go a bit more in-depth on gradient tracking. \n",
        "\n",
        "#### `requires_grad`: turning tracking on\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "6da23eec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w: tensor([ 0.3792,  1.6658,  0.2601,  0.4246, -0.5017], requires_grad=True)\n",
            "grad: tensor([ 0.3792,  1.6658,  0.2601,  0.4246, -0.5017])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "w = torch.randn(5, requires_grad=True)\n",
        "L = 0.5 * (w * w).sum()\n",
        "L.backward()\n",
        "\n",
        "print(\"w:\", w)\n",
        "print(\"grad:\", w.grad)\n",
        "\n",
        "# Output:\n",
        "# w: tensor([...], requires_grad=True)\n",
        "# grad: tensor([...])   # should equal w\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a494a1ef",
      "metadata": {},
      "source": [
        "\n",
        "#### Why you “can’t just assign into” a leaf tensor requiring grad\n",
        "\n",
        "A **leaf tensor** is one that was created by you (not the result of an operation) and has `requires_grad=True`. It is the starting node that accumulates gradients in `.grad`.\n",
        "\n",
        "If `w` is a leaf tensor with `requires_grad=True`, in-place modifications can break the recorded computation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "389b92cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "in-place assignment failed: a view of a leaf Variable that requires grad is being used in an in-place operation.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "w = torch.randn(3, requires_grad=True)\n",
        "\n",
        "try:\n",
        "    w[0] = 0.0\n",
        "except RuntimeError as e:\n",
        "    print(\"in-place assignment failed:\", e)\n",
        "\n",
        "# Output:\n",
        "# in-place assignment failed: a view of a leaf Variable that requires grad is being used in an in-place operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "826e33b0",
      "metadata": {},
      "source": [
        "\n",
        "A standard pattern is:\n",
        "\n",
        "1. create a tensor,\n",
        "2. do any preprocessing (normalization, clipping),\n",
        "3. then turn on grad tracking.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "b05d67d5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w.requires_grad: True\n",
            "w.grad: tensor([ 0.3349,  0.6936, -0.6377])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "w = torch.randn(3)\n",
        "w = w / w.norm()          # preprocessing while not tracking gradients\n",
        "w.requires_grad_()        # now start tracking\n",
        "\n",
        "L = 0.5 * (w * w).sum()\n",
        "L.backward()\n",
        "\n",
        "print(\"w.requires_grad:\", w.requires_grad)\n",
        "print(\"w.grad:\", w.grad)\n",
        "\n",
        "# Output:\n",
        "# w.requires_grad: True\n",
        "# w.grad: tensor([...])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cb72e85",
      "metadata": {},
      "source": [
        "\n",
        "#### `detach`: stop tracking, but keep the same storage\n",
        "\n",
        "`detach()` returns a tensor that shares memory with the original tensor, but is not connected to the computational graph.\n",
        "\n",
        "Two key questions:\n",
        "\n",
        "1. Does it point to the same memory?  \n",
        "2. If you modify the detached tensor, does the original change?\n",
        "\n",
        "Yes to both.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "b78ee752",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "same storage? True\n",
            "after modifying v:\n",
            "w: tensor([999.,   2.,   3.], requires_grad=True)\n",
            "v: tensor([999.,   2.,   3.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "w = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "v = w.detach()\n",
        "\n",
        "print(\"same storage?\", w.data_ptr() == v.data_ptr())\n",
        "\n",
        "v[0] = 999.0\n",
        "print(\"after modifying v:\")\n",
        "print(\"w:\", w)\n",
        "print(\"v:\", v)\n",
        "\n",
        "# Output:\n",
        "# same storage? True\n",
        "# after modifying v:\n",
        "# w: tensor([999.,   2.,   3.], requires_grad=True)\n",
        "# v: tensor([999.,   2.,   3.])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78f7c7a1",
      "metadata": {},
      "source": [
        "\n",
        "This is useful and dangerous.\n",
        "\n",
        "Useful when:\n",
        "\n",
        "- you want a non-tracked view for logging,\n",
        "- you want to stop gradient tracking for a subcomputation.\n",
        "\n",
        "Dangerous when:\n",
        "\n",
        "- you modify the detached tensor and accidentally mutate the tracked parameters.\n",
        "\n",
        "If you need a non-tracked copy that is safe to mutate, you want `detach().clone()`.\n",
        "\n",
        "#### `clone`: copy data\n",
        "\n",
        "We revisit `clone()` here because now we care about autograd, not just storage. The key point: `clone()` copies the data but **preserves the computation graph** when the source is tracked.\n",
        "\n",
        "`clone()` makes a new tensor with its own storage.\n",
        "\n",
        "If you clone a tracked tensor, the clone is still connected to the graph (it is not a leaf). Because it is non-leaf, its `.grad` is not populated by default; gradients accumulate on the original leaf unless you call `retain_grad()` on the clone.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "9d51666d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w.is_leaf: True\n",
            "c.is_leaf: False\n",
            "w.grad: tensor([ 3.2767, -1.3485, -0.7248])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "w = torch.randn(3, requires_grad=True)\n",
        "c = w.clone()\n",
        "\n",
        "print(\"w.is_leaf:\", w.is_leaf)\n",
        "print(\"c.is_leaf:\", c.is_leaf)\n",
        "\n",
        "L = (c * c).sum()\n",
        "L.backward()\n",
        "\n",
        "print(\"w.grad:\", w.grad)\n",
        "\n",
        "# Output:\n",
        "# w.is_leaf: True\n",
        "# c.is_leaf: False\n",
        "# w.grad: tensor([...])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "592e3fa0",
      "metadata": {},
      "source": [
        "\n",
        "So `clone()` is a data copy, but not a “stop gradient,” meaning gradients still flow back through it.\n",
        "\n",
        "If you want a copy that is disconnected from the graph:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "174026aa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "snap.requires_grad: False\n",
            "same storage? False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "w = torch.randn(3, requires_grad=True)\n",
        "snap = w.detach().clone()\n",
        "\n",
        "print(\"snap.requires_grad:\", snap.requires_grad)\n",
        "print(\"same storage?\", w.data_ptr() == snap.data_ptr())\n",
        "\n",
        "# Output:\n",
        "# snap.requires_grad: False\n",
        "# same storage? False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52d42929",
      "metadata": {},
      "source": [
        "\n",
        "#### When do we use `detach().clone()` in practice?\n",
        "\n",
        "Whenever you want to store values without storing the graph.\n",
        "\n",
        "Common patterns:\n",
        "\n",
        "- logging parameter snapshots during training,\n",
        "- exponential moving averages (EMA) of weights,\n",
        "\n",
        "\n",
        "#### Autodiff reminder: recorded operations + chain rule\n",
        "\n",
        "When you write a loss in PyTorch, you are giving PyTorch a computational graph.\n",
        "\n",
        "In math terms, you are expressing $L$ as a composition:\n",
        "\n",
        "$$\n",
        "L = g \\circ h \\circ r \\circ \\cdots\n",
        "$$\n",
        "\n",
        "PyTorch applies the multivariate chain rule through that recorded composition.\n",
        "\n",
        "We are not proving the multivariate chain rule here. If you want a clean Jacobian-based explanation, see:\n",
        "\n",
        "- <https://damek.github.io/STAT-4830/archive/2025/section/5/notes.html#extending-to-higher-dimensions-the-jacobian>\n",
        "\n",
        "### 4.6 Lingering question: what happens if `backward()` sees a non-scalar tensor?\n",
        "\n",
        "`backward()` computes derivatives of a scalar loss.\n",
        "\n",
        "One might think you can do:\n",
        "\n",
        "- compute a vector output,\n",
        "- call `backward()`.\n",
        "\n",
        "But a vector output does not have “the gradient” in the sense we need for optimization. It has a **Jacobian**.\n",
        "\n",
        "Definition: if $f:\\mathbb{R}^d \\to \\mathbb{R}^m$, the Jacobian is the $m \\times d$ matrix of partial derivatives\n",
        "$$\n",
        "J_{ij} = \\frac{\\partial f_i}{\\partial x_j}.\n",
        "$$\n",
        "\n",
        "PyTorch resolves this by defaulting to **vector–Jacobian products**.\n",
        "\n",
        "Concretely:\n",
        "\n",
        "- if `y` is non-scalar, `y.backward()` is ambiguous unless you specify a vector to multiply the Jacobian by.\n",
        "\n",
        "Here is the failure mode:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "6f9a1eec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "backward failed: grad can be implicitly created only for scalar outputs\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "w = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y = w * w   # y is vector-valued\n",
        "\n",
        "try:\n",
        "    y.backward()\n",
        "except RuntimeError as e:\n",
        "    print(\"backward failed:\", e)\n",
        "\n",
        "# Output:\n",
        "# backward failed: grad can be implicitly created only for scalar outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb54d2c6",
      "metadata": {},
      "source": [
        "\n",
        "With `backward`, you can pass a vector `v` and get a Jacobian–vector product.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "987328ce",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "w.grad: tensor([ 20.,  80., 180.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "w = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "y = w * w\n",
        "\n",
        "v = torch.tensor([10.0, 20.0, 30.0])   # same shape as y\n",
        "y.backward(v)\n",
        "\n",
        "print(\"w.grad:\", w.grad)\n",
        "\n",
        "# Output:\n",
        "# w.grad: tensor([ 20.,  80., 180.])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd59b3e",
      "metadata": {},
      "source": [
        "\n",
        "Example (quadratic vector):\n",
        "\n",
        "- $y = (w_1^2,w_2^2,w_3^2)$ has Jacobian $J=\\mathrm{diag}(2w_1,2w_2,2w_3)$,\n",
        "- `y.backward(v)` computes $J^\\top v$,\n",
        "- here that is $(2w_1 v_1,2w_2 v_2,2w_3 v_3)$.\n",
        "\n",
        "### 4.7 Dtypes: precision, speed, and accumulation\n",
        "\n",
        "PyTorch supports multiple numeric types.\n",
        "\n",
        "Common ones:\n",
        "\n",
        "- floating point: `float16`, `bfloat16`, `float32`, `float64`,\n",
        "- integers: `int32`, `int64`,\n",
        "- booleans: `bool`.\n",
        "\n",
        "\n",
        "What does “precision” mean?\n",
        "\n",
        "A floating-point number stores three fields: sign, exponent, and mantissa (significand). The exponent sets the scale (where the decimal point sits); the mantissa stores the significant digits. Different float types allocate bits differently.\n",
        "\n",
        "**Figure 3.8: floating-point bit layouts (sign, exponent, mantissa).**  \n",
        "![Floating-point bit layouts](figures/float_bit_layout.png)  \n",
        "*Figure 3.8: The exponent controls scale (range); the mantissa controls precision. bfloat16 keeps the float32 exponent width but fewer mantissa bits.*\n",
        "\n",
        "Lower precision means fewer mantissa bits. That cuts memory and often increases throughput on modern accelerators, but it increases rounding error and can make some computations inaccurate.\n",
        "\n",
        "When training large language models, it is common for a model to have many groups of parameters, and each group can use a different precision level.\n",
        "\n",
        "Figuring out which precision is write for your problem is a matter of trial and error. One practical rule of thumb that always applies is to keep accumulators in high precision. What's an accumulator? It is a variable that holds a running sum (loss totals, gradient sums), and rounding error compounds there.\n",
        "  \n",
        "\n",
        "#### Accumulation exercise (float16 vs float32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "80d675e0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "float32 accumulator + float32 values: tensor(10.0002)\n",
            "float16 accumulator + float16 values: tensor(9.9531, dtype=torch.float16)\n",
            "float32 accumulator + float16 values (implicit upcast): tensor(10.0021)\n",
            "float32 accumulator + float16 values (explicit upcast): tensor(10.0021)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# 1000 copies of ~0.01 should sum to ~10.\n",
        "n = 1000\n",
        "vals32 = torch.full((n,), 0.0100001, dtype=torch.float32)\n",
        "vals16 = vals32.to(torch.float16)\n",
        "\n",
        "# float32 accumulator + float32 values\n",
        "acc32_32 = torch.tensor(0.0, dtype=torch.float32)\n",
        "for v in vals32:\n",
        "    acc32_32 = acc32_32 + v\n",
        "\n",
        "# float16 accumulator + float16 values\n",
        "acc16_16 = torch.tensor(0.0, dtype=torch.float16)\n",
        "for v in vals16:\n",
        "    acc16_16 = acc16_16 + v\n",
        "\n",
        "# float32 accumulator + float16 values (implicit upcast of v during addition)\n",
        "acc32_16 = torch.tensor(0.0, dtype=torch.float32)\n",
        "for v in vals16:\n",
        "    acc32_16 = acc32_16 + v\n",
        "\n",
        "# float32 accumulator + float16 values (explicit upcast)\n",
        "acc32_16_explicit = torch.tensor(0.0, dtype=torch.float32)\n",
        "for v in vals16:\n",
        "    acc32_16_explicit = acc32_16_explicit + v.to(torch.float32)\n",
        "\n",
        "print(\"float32 accumulator + float32 values:\", acc32_32)\n",
        "print(\"float16 accumulator + float16 values:\", acc16_16)\n",
        "print(\"float32 accumulator + float16 values (implicit upcast):\", acc32_16)\n",
        "print(\"float32 accumulator + float16 values (explicit upcast):\", acc32_16_explicit)\n",
        "\n",
        "# Output:\n",
        "# float32 accumulator + float32 values: tensor(10.0001)\n",
        "# float16 accumulator + float16 values: tensor(9.9531, dtype=torch.float16)\n",
        "# float32 accumulator + float16 values (implicit upcast): tensor(10.0021)\n",
        "# float32 accumulator + float16 values (explicit upcast): tensor(10.0021)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9373bc1e",
      "metadata": {},
      "source": [
        "\n",
        "Interpretation:\n",
        "\n",
        "- `float16` cannot represent `0.0100001` exactly; it rounds it.\n",
        "- `float16` accumulation loses additional information because the accumulator itself cannot represent intermediate sums finely.\n",
        "- `float32` accumulation avoids the accumulator-rounding issue, but you still sum the rounded `float16` values, so you can end up slightly high.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3190947a",
      "metadata": {},
      "source": [
        "\n",
        "### 4.8 Efficiency issues\n",
        "\n",
        "Throughout this section, we will:\n",
        "\n",
        "- do the same computation two ways,\n",
        "- time them,\n",
        "- explain why one is better.\n",
        "\n",
        "Timing is hardware-dependent. You should focus on the order-of-magnitude differences and the reasons, not the exact numbers.\n",
        "\n",
        "#### In-place operations: the `_` suffix\n",
        "\n",
        "An **in-place** operation writes its result into the existing tensor’s memory instead of allocating a new tensor.\n",
        "\n",
        "Many PyTorch operations have an in-place version with a trailing underscore.\n",
        "\n",
        "Example: `add_` modifies a tensor in place.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "601552dd",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out-of-place: tensor([11., 22., 33.])\n",
            "x unchanged: tensor([1., 2., 3.])\n",
            "in-place: tensor([11., 22., 33.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "y = torch.tensor([10.0, 20.0, 30.0])\n",
        "\n",
        "x_out = x + y\n",
        "print(\"out-of-place:\", x_out)\n",
        "print(\"x unchanged:\", x)\n",
        "\n",
        "x_in = x.clone()\n",
        "x_in.add_(y)\n",
        "print(\"in-place:\", x_in)\n",
        "\n",
        "# Output:\n",
        "# out-of-place: tensor([11., 22., 33.])\n",
        "# x unchanged: tensor([1., 2., 3.])\n",
        "# in-place: tensor([11., 22., 33.])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "445b5390",
      "metadata": {},
      "source": [
        "\n",
        "Why in-place?\n",
        "\n",
        "- it avoids allocating a new tensor,\n",
        "- it can reduce memory traffic.\n",
        "\n",
        "One might think “always use in-place for speed.” However:\n",
        "\n",
        "- in-place ops can break autodiff if they overwrite values needed for backward,\n",
        "- in-place ops can make code harder to reason about.\n",
        "\n",
        "In training loops, the common safe in-place updates happen under `torch.no_grad()`.\n",
        "\n",
        "Finally, note that `x = x + y` is **out-of-place** (it allocates a new tensor and rebinds `x`).\n",
        "\n",
        "#### Broadcasting: matrix–vector Hadamard product\n",
        "\n",
        "Broadcasting is a rule that lets PyTorch align tensors with different shapes by “stretching” size‑1 dimensions, without allocating the repeated data.\n",
        "\n",
        "In this section we give a row‑wise example: a vector multiplies each row of a matrix (Hadamard product).\n",
        "\n",
        "We want to compute:\n",
        "\n",
        "- matrix $A \\in \\mathbb{R}^{n \\times d}$,\n",
        "- vector $v \\in \\mathbb{R}^d$,\n",
        "- result $B \\in \\mathbb{R}^{n \\times d}$ where $B_{ij} = A_{ij} v_j$.\n",
        "\n",
        "There are three conceptual ways:\n",
        "\n",
        "1. Python for-loop (slow, sequential),\n",
        "2. explicitly repeat the vector into a matrix (extra memory),\n",
        "3. broadcasting (no extra memory for the repeated matrix).\n",
        "\n",
        "**Figure 3.4: broadcasting as “virtual replication.”**  \n",
        "![Broadcasting matrix-vector Hadamard](figures/broadcasting_matrix_vector.png)\n",
        "*Figure 3.4: Broadcasting behaves like repeating a vector across rows, but without allocating the repeated matrix.*\n",
        "\n",
        "Code:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "db11d074",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A.shape: torch.Size([4, 3])\n",
            "v.shape: torch.Size([3])\n",
            "B.shape: torch.Size([4, 3])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "n = 4\n",
        "d = 3\n",
        "\n",
        "A = torch.randn(n, d)\n",
        "v = torch.randn(d)\n",
        "\n",
        "B = A * v   # broadcasting: v treated as shape (1,d) and expanded across rows\n",
        "\n",
        "print(\"A.shape:\", A.shape)\n",
        "print(\"v.shape:\", v.shape)\n",
        "print(\"B.shape:\", B.shape)\n",
        "\n",
        "# Output:\n",
        "# A.shape: torch.Size([4, 3])\n",
        "# v.shape: torch.Size([3])\n",
        "# B.shape: torch.Size([4, 3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4293a6b",
      "metadata": {},
      "source": [
        "\n",
        "Why is broadcasting more efficient?\n",
        "\n",
        "- it does not allocate an $n \\times d$ “tiled” copy of `v`,\n",
        "- it lets a single kernel implement the computation in parallel,\n",
        "- the expanded dimensions are represented by stride metadata, not by actual memory.\n",
        "\n",
        "#### Vectorization beats Python loops\n",
        "\n",
        "Python loops are slow. The reason is:\n",
        "\n",
        "- each loop iteration is interpreted, meaning Python executes each iteration step-by-step instead of a compiled kernel,\n",
        "- each tensor operation call has overhead,\n",
        "- you prevent PyTorch from launching efficient kernels over large blocks of data.\n",
        "\n",
        "A long timed example: compute $\\sum_i w_i^2$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "ebf8868b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loop sum: 4997394.784284724\n",
            "vec  sum: 4997395.0\n",
            "loop time (s): 9.1332550420193\n",
            "vec  time (s): 0.0006089590024203062\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "n = 5_000_000\n",
        "w = torch.randn(n)\n",
        "\n",
        "# Python loop\n",
        "t0 = time.perf_counter()\n",
        "s = 0.0\n",
        "for i in range(n):\n",
        "    s += float(w[i] * w[i])\n",
        "t1 = time.perf_counter()\n",
        "\n",
        "# Vectorized\n",
        "t2 = time.perf_counter()\n",
        "s2 = torch.sum(w * w).item()\n",
        "t3 = time.perf_counter()\n",
        "\n",
        "print(\"loop sum:\", s)\n",
        "print(\"vec  sum:\", s2)\n",
        "print(\"loop time (s):\", t1 - t0)\n",
        "print(\"vec  time (s):\", t3 - t2)\n",
        "\n",
        "# Example output (your times will differ):\n",
        "# loop sum: 4999584.123...\n",
        "# vec  sum: 4999584.125...\n",
        "# loop time (s): 2.80\n",
        "# vec  time (s): 0.02\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "677884b5",
      "metadata": {},
      "source": [
        "\n",
        "The numeric values differ slightly because:\n",
        "\n",
        "- the loop casts each term to Python float and accumulates in Python,\n",
        "- the vectorized reduction uses PyTorch’s reduction implementation and dtype rules.\n",
        "\n",
        "\n",
        "#### How memory layout influences efficiency\n",
        "\n",
        "A cache is small, fast memory near the CPU that holds recently accessed data. One might think memory layout is just bookkeeping. However, it changes which elements are neighbors in memory, which changes cache behavior.\n",
        "\n",
        "When the CPU loads a cache line into the cache, it brings nearby elements with it. For row-major data, “nearby” means adjacent columns in the same row. For column-major data, “nearby” means adjacent rows in the same column.\n",
        "\n",
        "That means the same computation can run at different speeds depending on access order. To make this concrete, we sum the same row-major tensor in two ways: along rows (contiguous access) and along columns (strided access). We fix the number of rows and scale the number of columns so the stride gap grows. We average each timing over a few repeats to reduce noise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "fd93f708",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 650x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.set_num_threads(1)\n",
        "\n",
        "def time_sum(A, dim, repeats=3):\n",
        "    t0 = time.perf_counter()\n",
        "    for _ in range(repeats):\n",
        "        _ = torch.sum(A, dim=dim)\n",
        "    t1 = time.perf_counter()\n",
        "    return t1 - t0\n",
        "\n",
        "rows = 512\n",
        "cols_list = [512, 1024, 2048, 4096, 8192, 16384, 32768]\n",
        "t_row, t_col = [], []\n",
        "\n",
        "for cols in cols_list:\n",
        "    A = torch.randn(rows, cols)\n",
        "    t_row.append(time_sum(A, dim=1))  # contiguous\n",
        "    t_col.append(time_sum(A, dim=0))  # strided\n",
        "\n",
        "plt.figure(figsize=(6.5, 4.0))\n",
        "plt.plot(cols_list, t_row, label=\"sum along rows (contiguous)\")\n",
        "plt.plot(cols_list, t_col, label=\"sum along columns (strided)\")\n",
        "plt.xlabel(\"columns m (512 x m)\")\n",
        "plt.ylabel(\"time (seconds)\")\n",
        "plt.title(\"Row-major tensor: row-sum vs column-sum\")\n",
        "plt.grid(True, alpha=0.25)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"figures/memory_layout_efficiency.png\", dpi=200, bbox_inches=\"tight\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd08f027",
      "metadata": {},
      "source": [
        "\n",
        "The row-major layout is faster because the inner loop hits contiguous memory. The column-major layout forces strided access and more cache misses.\n",
        "\n",
        "This loop is intentionally slow to isolate the memory layout effect. In real code, you should use vectorized kernels.\n",
        "\n",
        "![Row-major loop time by memory layout](figures/memory_layout_efficiency.png)\n",
        "*Figure 3.7: Row-major access is faster on row-major data because the inner loop walks contiguous memory. On column-major data the same loop jumps by a stride and slows down as the matrix grows.*\n",
        "\n",
        "#### Matmul order: $A(BC)$ vs $(AB)C$\n",
        "\n",
        "Matrix multiplication is associative mathematically, but not computationally.\n",
        "\n",
        "Let:\n",
        "\n",
        "- $A \\in \\mathbb{R}^{m \\times n}$,\n",
        "- $B \\in \\mathbb{R}^{n \\times p}$,\n",
        "- $C \\in \\mathbb{R}^{p \\times q}$.\n",
        "\n",
        "The flop counts differ:\n",
        "\n",
        "- $(AB)C$ costs about $mnp + mpq$ multiply-adds,\n",
        "- $A(BC)$ costs about $npq + mnq$ multiply-adds.\n",
        "\n",
        "The difference can be huge.\n",
        "\n",
        "**Figure 3.6: associativity changes intermediate sizes.**  \n",
        "![Matmul associativity cost](figures/matmul_associativity_cost.png)\n",
        "*Figure 3.6: The intermediate matrix in $(AB)C$ has shape $m \\times p$. The intermediate in $A(BC)$ has shape $n \\times q$. You want the smaller intermediate when possible.*\n",
        "\n",
        "A concrete timing example (average over 3 runs):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "83f73378",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max diff: 0.000457763671875\n",
            "avg (AB)C time (s): 0.00012645800597965717\n",
            "avg A(BC) time (s): 0.0002581806620582938\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "m, n, p, q = 512, 512, 8, 512\n",
        "\n",
        "A = torch.randn(m, n)\n",
        "B = torch.randn(n, p)\n",
        "C = torch.randn(p, q)\n",
        "\n",
        "def avg_time(fn, repeats=3):\n",
        "    times = []\n",
        "    out = None\n",
        "    for _ in range(repeats):\n",
        "        t0 = time.perf_counter()\n",
        "        out = fn()\n",
        "        t1 = time.perf_counter()\n",
        "        times.append(t1 - t0)\n",
        "    return sum(times) / len(times), out\n",
        "\n",
        "t_ab, X1 = avg_time(lambda: (A @ B) @ C, repeats=3)\n",
        "t_bc, X2 = avg_time(lambda: A @ (B @ C), repeats=3)\n",
        "\n",
        "print(\"max diff:\", (X1 - X2).abs().max().item())\n",
        "print(\"avg (AB)C time (s):\", t_ab)\n",
        "print(\"avg A(BC) time (s):\", t_bc)\n",
        "\n",
        "# Example output (your times will differ):\n",
        "# max diff: 0.0\n",
        "# avg (AB)C time (s): 0.03\n",
        "# avg A(BC) time (s): 0.35\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5006b6de",
      "metadata": {},
      "source": [
        "\n",
        "The outputs match (up to floating point), but runtime differs because you changed the cost profile.\n",
        "\n",
        "One might think “PyTorch will figure this out automatically.” However, matrix multiplication order is not generally optimized away for you. You must choose the parentheses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0a9e58",
      "metadata": {},
      "outputs": [],
      "source": [
        "# accumulation exercise moved above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59208898",
      "metadata": {},
      "source": [
        "\n",
        "### 4.9 Devices: CPU, GPU, MPS, TPU\n",
        "\n",
        "By default, tensors live on CPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "c80d395d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.randn(3)\n",
        "print(\"device:\", x.device)\n",
        "\n",
        "# Output:\n",
        "# device: cpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50b84df9",
      "metadata": {},
      "source": [
        "\n",
        "You can move tensors to a device with `.to(device)`. In PyTorch, `cuda` is used to name NVIDIA GPUs (e.g., `cuda:0` for the first GPU).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "4750f44c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "x = torch.randn(3).to(device)\n",
        "\n",
        "print(\"device:\", x.device)\n",
        "\n",
        "# Example output:\n",
        "# device: cuda:0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0e2ad74",
      "metadata": {},
      "source": [
        "\n",
        "Newer Macs may have an `mps` device. In my experience, MPS support is fragile! If you are on a Mac and you hit mysterious runtime errors, the first debugging step is often: run on CPU or on a CUDA machine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "id": "6d3a66c9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: mps:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "x = torch.randn(3).to(device)\n",
        "print(\"device:\", x.device)\n",
        "\n",
        "# Example output:\n",
        "# device: mps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Other devices exist (TPUs, etc.), typically via additional libraries (e.g., `torch_xla` in Colab). We will not rely on them in this course.\n",
        "\n",
        "We'll return to this later on in the course, but if you want a concrete GPU mental model, see:\n",
        "\n",
        "- <https://damek.github.io/random/basic-facts-about-gpus/>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
